{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1754f3e",
   "metadata": {},
   "source": [
    "<H1>Realevancy Checker Model </H1>\n",
    "\n",
    "Group: 6<br>\n",
    "Prepared by: Dunith and Jun Ming (Julian)<br>\n",
    "Date: 22/8/2021\n",
    "\n",
    "Trains a model to predict the relevancy of an article with the aid of CORD-19 data set and judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2443cdf",
   "metadata": {},
   "source": [
    "<h2>Model Training with CORD-19 Data set</h2> <br>\n",
    "1 - Imports and read in dataset <br>\n",
    "2 - Text Processing <br> \n",
    "3 - Keywords Extraction <br>\n",
    "4 - Features engineering <br>\n",
    "5 - Model training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d9d1f",
   "metadata": {},
   "source": [
    "<h5> 1 - Import necessary libraries and packages, reads in dataset </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1728d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import re\n",
    "import random\n",
    "random.seed(2021)\n",
    "\n",
    "# Import dataset\n",
    "# dataset is cleaned and structured for easy operations\n",
    "df_train = pd.read_excel('cord-data.xlsx')  \n",
    "#df_train = df_train.dropna()    #drops the NA values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499b1a8",
   "metadata": {},
   "source": [
    "<h5> 2 - Text processing such as removing stop words, lemmatization and stemming </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3755d432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>round</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>full_text</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coronaviru origin</td>\n",
       "      <td>2</td>\n",
       "      <td>6ml0puh8</td>\n",
       "      <td>1</td>\n",
       "      <td>Review\\n\\nMiddle East respiratory syndrome\\nco...</td>\n",
       "      <td>review middl east respiratori syndrom coronavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coronaviru origin</td>\n",
       "      <td>2</td>\n",
       "      <td>0khg28ex</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronaviru origin</td>\n",
       "      <td>2</td>\n",
       "      <td>16rgt4ca</td>\n",
       "      <td>0</td>\n",
       "      <td>Licciardi et al. Pediatric Rheumatology\\n(2020...</td>\n",
       "      <td>licciardi et al. pediatr rheumatolog  2020  18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coronaviru origin</td>\n",
       "      <td>2</td>\n",
       "      <td>1bvsn9e8</td>\n",
       "      <td>1</td>\n",
       "      <td>Pathogens and Disease, 77, 2019, ftaa006\\ndoi:...</td>\n",
       "      <td>pathogen and disease  77  2019  ftaa006 doi  1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coronaviru respons to weather chang</td>\n",
       "      <td>2</td>\n",
       "      <td>03id5o2g</td>\n",
       "      <td>0</td>\n",
       "      <td>ARTICLE\\nhttps://doi.org/10.1038/s41467-019-13...</td>\n",
       "      <td>articl https //doi.org/10.1038/s41467 019 1365...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>coronaviru recoveri</td>\n",
       "      <td>2</td>\n",
       "      <td>bzc7luwj</td>\n",
       "      <td>1</td>\n",
       "      <td>medRxiv preprint doi: https://doi.org/10.1101/...</td>\n",
       "      <td>medrxiv preprint doi  https //doi.org/10.1101/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>coronaviru recoveri</td>\n",
       "      <td>2</td>\n",
       "      <td>c4bera7b</td>\n",
       "      <td>0</td>\n",
       "      <td>Research in Social and Administrative Pharmacy...</td>\n",
       "      <td>research in social and administr pharmaci 17  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>coronaviru public dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>yq05djrc</td>\n",
       "      <td>1</td>\n",
       "      <td>TRANSLATIONAL KNOWLEDGE MAP OF COVID-19\\nCesar...</td>\n",
       "      <td>translat knowledg map of covid 19 cesar aguado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>coronaviru public dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>z0bkpmpk</td>\n",
       "      <td>1</td>\n",
       "      <td>arXiv:2004.04569v2 [eess.IV] 9 May 2020\\n\\nCOV...</td>\n",
       "      <td>arxiv 2004.04569 volt. 2  eess.iv  9 may 2020 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>coronaviru public dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>z4q0eyjo</td>\n",
       "      <td>1</td>\n",
       "      <td>medRxiv preprint doi: https://doi.org/10.1101/...</td>\n",
       "      <td>medrxiv preprint doi  https //doi.org/10.1101/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  round    doc_id  relevancy  \\\n",
       "0                      coronaviru origin      2  6ml0puh8          1   \n",
       "1                      coronaviru origin      2  0khg28ex          0   \n",
       "2                      coronaviru origin      2  16rgt4ca          0   \n",
       "3                      coronaviru origin      2  1bvsn9e8          1   \n",
       "4    coronaviru respons to weather chang      2  03id5o2g          0   \n",
       "..                                   ...    ...       ...        ...   \n",
       "130                  coronaviru recoveri      2  bzc7luwj          1   \n",
       "131                  coronaviru recoveri      2  c4bera7b          0   \n",
       "132            coronaviru public dataset      2  yq05djrc          1   \n",
       "133            coronaviru public dataset      2  z0bkpmpk          1   \n",
       "134            coronaviru public dataset      2  z4q0eyjo          1   \n",
       "\n",
       "                                             full_text  \\\n",
       "0    Review\\n\\nMiddle East respiratory syndrome\\nco...   \n",
       "1                                                  NaN   \n",
       "2    Licciardi et al. Pediatric Rheumatology\\n(2020...   \n",
       "3    Pathogens and Disease, 77, 2019, ftaa006\\ndoi:...   \n",
       "4    ARTICLE\\nhttps://doi.org/10.1038/s41467-019-13...   \n",
       "..                                                 ...   \n",
       "130  medRxiv preprint doi: https://doi.org/10.1101/...   \n",
       "131  Research in Social and Administrative Pharmacy...   \n",
       "132  TRANSLATIONAL KNOWLEDGE MAP OF COVID-19\\nCesar...   \n",
       "133  arXiv:2004.04569v2 [eess.IV] 9 May 2020\\n\\nCOV...   \n",
       "134  medRxiv preprint doi: https://doi.org/10.1101/...   \n",
       "\n",
       "                                              document  \n",
       "0    review middl east respiratori syndrom coronavi...  \n",
       "1                                                 null  \n",
       "2    licciardi et al. pediatr rheumatolog  2020  18...  \n",
       "3    pathogen and disease  77  2019  ftaa006 doi  1...  \n",
       "4    articl https //doi.org/10.1038/s41467 019 1365...  \n",
       "..                                                 ...  \n",
       "130  medrxiv preprint doi  https //doi.org/10.1101/...  \n",
       "131  research in social and administr pharmaci 17  ...  \n",
       "132  translat knowledg map of covid 19 cesar aguado...  \n",
       "133  arxiv 2004.04569 volt. 2  eess.iv  9 may 2020 ...  \n",
       "134  medrxiv preprint doi  https //doi.org/10.1101/...  \n",
       "\n",
       "[135 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def str_stem(s): \n",
    "    \"\"\"\n",
    "    This function cleans the text in article such as removing stop words, stemming, standalize the cases and so on\n",
    "    :param s: text / article\n",
    "    :return: return processed texts\n",
    "    \"\"\" \n",
    "    if isinstance(s, str):\n",
    "        s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(°|degrees|degree)\\.?\", r\"\\1 deg. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(v|volts|volt)\\.?\", r\"\\1 volt. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(wattage|watts|watt)\\.?\", r\"\\1 watt. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1 amp. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(qquart|quart)\\.?\", r\"\\1 qt. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(hours|hour|hrs.)\\.?\", r\"\\1 hr \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(gallons per minute|gallon per minute|gal per minute|gallons/min.|gallons/min)\\.?\", r\"\\1 gal. per min. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(gallons per hour|gallon per hour|gal per hour|gallons/hour|gallons/hr)\\.?\", r\"\\1 gal. per hr \", s)\n",
    "        # Deal with special characters\n",
    "        s = s.replace(\"$\",\" \")\n",
    "        s = s.replace(\"?\",\" \")\n",
    "        s = s.replace(\"&nbsp;\",\" \")\n",
    "        s = s.replace(\"&amp;\",\"&\")\n",
    "        s = s.replace(\"&#39;\",\"'\")\n",
    "        s = s.replace(\"/>/Agt/>\",\"\")\n",
    "        s = s.replace(\"</a<gt/\",\"\")\n",
    "        s = s.replace(\"gt/>\",\"\")\n",
    "        s = s.replace(\"/>\",\"\")\n",
    "        s = s.replace(\"<br\",\"\")\n",
    "        s = s.replace(\"<.+?>\",\"\")\n",
    "        s = s.replace(\"[ &<>)(_,;:!?\\+^~@#\\$]+\",\" \")\n",
    "        s = s.replace(\"'s\\\\b\",\"\")\n",
    "        s = s.replace(\"[']+\",\"\")\n",
    "        s = s.replace(\"[\\\"]+\",\"\")\n",
    "        s = s.replace(\"-\",\" \")\n",
    "        s = s.replace(\"+\",\" \")\n",
    "        # Remove text between paranthesis/brackets)\n",
    "        s = s.replace(\"[ ]?[[(].+?[])]\",\"\")\n",
    "        # remove sizes\n",
    "        s = s.replace(\"size: .+$\",\"\")\n",
    "        s = s.replace(\"size [0-9]+[.]?[0-9]+\\\\b\",\"\")\n",
    "        \n",
    "        \n",
    "        return \" \".join([stemmer.stem(re.sub('[^A-Za-z0-9-./]', ' ', word)) for word in s.lower().split()])\n",
    "    else:\n",
    "        return \"null\"\n",
    "    \n",
    "# clean the query term and the doc/article\n",
    "df_train['query'] = df_train['query'].apply(str_stem)\n",
    "df_train['document'] = df_train['full_text'].apply(str_stem)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11899090",
   "metadata": {},
   "source": [
    "<h5> 3 - Extracts the keywords from article by using TF-IDF </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c59485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doi</th>\n",
       "      <td>0.789979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https</th>\n",
       "      <td>0.394989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medrxiv</th>\n",
       "      <td>0.332992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preprint</th>\n",
       "      <td>0.330202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>among</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>although</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1154 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             tfidf\n",
       "doi       0.789979\n",
       "https     0.394989\n",
       "medrxiv   0.332992\n",
       "preprint  0.330202\n",
       "02        0.000000\n",
       "...            ...\n",
       "analysi   0.000000\n",
       "an        0.000000\n",
       "among     0.000000\n",
       "although  0.000000\n",
       "zone      0.000000\n",
       "\n",
       "[1154 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    " \n",
    "\n",
    "def compute_tf_idf(corpus):\n",
    "    \"\"\"\n",
    "    This function computes the TF-IDF of the article to extract the keywords from the article.\n",
    "    :param corpus: article/document\n",
    "    :return: a table of keywords with its tf-idf values \n",
    "    \"\"\"\n",
    "    \n",
    "    docs = corpus.split('.')\n",
    "    \n",
    "    #instantiate CountVectorizer() \n",
    "    cv=CountVectorizer() \n",
    "\n",
    "    # this steps generates word counts for the words in your docs \n",
    "    word_count_vector=cv.fit_transform(docs)\n",
    "\n",
    "\n",
    "    word_count_vector.shape\n",
    "\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "    # print idf values \n",
    "    df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    "\n",
    "    # sort ascending \n",
    "    df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "    # count matrix \n",
    "    count_vector=cv.transform(docs) \n",
    "\n",
    "    # tf-idf scores \n",
    "    tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "\n",
    "    feature_names = cv.get_feature_names() \n",
    "    \n",
    "    #get tfidf vector for first document \n",
    "    first_document_vector=tf_idf_vector[0] \n",
    "\n",
    "    \n",
    "    #print the scores \n",
    "    df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "    res = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "# process all documents in the dataset\n",
    "all_corpus = df_train.loc[:,'document']\n",
    "\n",
    "# Creates new cols of keyword 1 to keyword 5 for each row\n",
    "for i in range(len(all_corpus)):\n",
    "    corpus = all_corpus[i]\n",
    "    if corpus != 'null':\n",
    "        tf_idf = compute_tf_idf(all_corpus[i])\n",
    "        features = tf_idf.index[0:5]\n",
    "        df_train.at[i,'keyword_1'] = features[0]\n",
    "        df_train.at[i,'keyword_2'] = features[1]\n",
    "        df_train.at[i,'keyword_3'] = features[2]\n",
    "        df_train.at[i,'keyword_4'] = features[3]\n",
    "        df_train.at[i,'keyword_5'] = features[4]\n",
    "        \n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34205010",
   "metadata": {},
   "source": [
    "<h5> 4 - Engineers the keywords into features for prediction </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122d93b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_len_of_query'] = df_train['query'].apply(lambda x:len(x.split())).astype(np.int64)\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_len_of_document'] = df_train['document'].apply(lambda x:len(x.split())).astype(np.int64)\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_len_of_kw1'] = df_train['keyword_1'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_len_of_kw2'] = df_train['keyword_2'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_len_of_kw3'] = df_train['keyword_3'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_len_of_kw4'] = df_train['keyword_4'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_len_of_kw5'] = df_train['keyword_5'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['total_info'] = df_train['query']+\"\\t\"+df_train['document']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['query_in_document'] = df_train['total_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_in_document'] = df_train['total_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['query_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_query']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['ratio_document'] = df_train['word_in_document']/df_train['word_len_of_query']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['keyword_doc1'] = df_train['keyword_1']+\"\\t\"+df_train['document']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw1_in_document'] = df_train['keyword_doc1'].apply(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_in_document'] = df_train['keyword_doc1'].apply(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw1_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_kw1']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw1_ratio_document'] = df_train['word_in_document']/df_train['word_len_of_kw1']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['keyword_doc2'] = df_train['keyword_2']+\"\\t\"+df_train['document']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw2_in_document'] = df_train['keyword_doc2'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_in_document'] = df_train['keyword_doc2'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw2_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_kw2']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw2_ratio_document'] = df_train['word_in_document']/df_train['word_len_of_kw2']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['keyword_doc3'] = df_train['keyword_3']+\"\\t\"+df_train['document']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw3_in_document'] = df_train['keyword_doc3'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_in_document'] = df_train['keyword_doc3'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw3_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_kw3']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw3_ratio_document'] = df_train['word_in_document']/df_train['word_len_of_kw3']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['keyword_doc4'] = df_train['keyword_4']+\"\\t\"+df_train['document']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw4_in_document'] = df_train['keyword_doc4'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_in_document'] = df_train['keyword_doc4'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw4_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_kw4']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw4_ratio_document'] = df_train['word_in_document']/df_train['word_len_of_kw4']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['keyword_doc5'] = df_train['keyword_5']+\"\\t\"+df_train['document']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw5_in_document'] = df_train['keyword_doc5'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['word_in_document'] = df_train['keyword_doc5'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw5_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_kw5']\n",
      "/var/folders/3d/0mm5wkcn54xd9wx7q9nzfqg00000gn/T/ipykernel_69304/2398617913.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['kw5_ratio_document'] = df_train['word_in_document']/df_train['word_len_of_kw5']\n"
     ]
    }
   ],
   "source": [
    "def str_common_word(str1, str2):\n",
    "    \"\"\"\n",
    "    Counts the common word in two strings\n",
    "    :param str1: a source string\n",
    "    :param str2: a target string\n",
    "    :return: the number of times of the common word\n",
    "    \"\"\"\n",
    "    str1, str2 = str1.lower(), str2.lower()\n",
    "    words, count = str1.split(), 0\n",
    "    for word in words:\n",
    "        if str2.find(word)>=0:\n",
    "            count+=1\n",
    "    return count\n",
    "    \n",
    "def str_whole_word(str1, str2, i_):\n",
    "    \"\"\"\n",
    "    Counts the whole word from the source string in the target string\n",
    "    :param str1: a source string\n",
    "    :param str2: a target string\n",
    "    :param i_: index of the search\n",
    "    :return: the number of times of the found whole word\n",
    "    \"\"\"\n",
    "    str1, str2 = str1.lower().strip(), str2.lower().strip()\n",
    "    count = 0\n",
    "    while i_ < len(str2):\n",
    "        i_ = str2.find(str1, i_)\n",
    "        if i_ == -1:\n",
    "            return count\n",
    "        else:\n",
    "            count += 1\n",
    "            i_ += len(str1)\n",
    "    return count\n",
    "\n",
    "\n",
    "# Drops the rows with NA values\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "\n",
    "# Creates new columns of the features of the query term, doc, and from keyword 1 to keyword 5\n",
    "df_train['word_len_of_query'] = df_train['query'].apply(lambda x:len(x.split())).astype(np.int64)\n",
    "df_train['word_len_of_document'] = df_train['document'].apply(lambda x:len(x.split())).astype(np.int64)\n",
    "df_train['word_len_of_kw1'] = df_train['keyword_1'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
    "df_train['word_len_of_kw2'] = df_train['keyword_2'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
    "df_train['word_len_of_kw3'] = df_train['keyword_3'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
    "df_train['word_len_of_kw4'] = df_train['keyword_4'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
    "df_train['word_len_of_kw5'] = df_train['keyword_5'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
    "\n",
    "\n",
    "# Query & Document\n",
    "# Create a new column that combine \"query\" and \"total_document\" \n",
    "df_train['total_info'] = df_train['query']+\"\\t\"+df_train['document'] \n",
    "# Number of times the entire search term appears in total document. \n",
    "df_train['query_in_document'] = df_train['total_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\n",
    "# Number of words that appear in search term also appear in total document.\n",
    "df_train['word_in_document'] = df_train['total_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "# The ratio of total document word length to search term word length\n",
    "df_train['query_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_query']\n",
    "# The ratio of total document and search term common word count to search term word count\n",
    "df_train['ratio_document'] = df_train['word_in_document']/df_train['word_len_of_query']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Keyword 1 & Document\n",
    "# Create a new column that combine \"query\" and \"total_document\" \n",
    "df_train['keyword_doc1'] = df_train['keyword_1']+\"\\t\"+df_train['document'] \n",
    "# Number of times the entire search term appears in total document. \n",
    "df_train['kw1_in_document'] = df_train['keyword_doc1'].apply(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
    "# Number of words that appear in search term also appear in total document.\n",
    "df_train['word_in_document'] = df_train['keyword_doc1'].apply(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
    "# The ratio of total document word length to search term word length\n",
    "df_train['kw1_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_kw1']\n",
    "# The ratio of total document and search term common word count to search term word count\n",
    "df_train['kw1_ratio_document'] = df_train['word_in_document']/df_train['word_len_of_kw1']\n",
    "\n",
    "\n",
    "# Keyword 2 & Document\n",
    "# Create a new column that combine \"query\" and \"total_document\" \n",
    "df_train['keyword_doc2'] = df_train['keyword_2']+\"\\t\"+df_train['document'] \n",
    "# Number of times the entire search term appears in total document. \n",
    "df_train['kw2_in_document'] = df_train['keyword_doc2'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
    "# Number of words that appear in search term also appear in total document.\n",
    "df_train['word_in_document'] = df_train['keyword_doc2'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
    "# The ratio of total document word length to search term word length\n",
    "df_train['kw2_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_kw2']\n",
    "# The ratio of total document and search term common word count to search term word count\n",
    "df_train['kw2_ratio_document'] = df_train['word_in_document']/df_train['word_len_of_kw2']\n",
    "\n",
    "\n",
    "# Keyword 3 & Document\n",
    "# Create a new column that combine \"query\" and \"total_document\" \n",
    "df_train['keyword_doc3'] = df_train['keyword_3']+\"\\t\"+df_train['document'] \n",
    "# Number of times the entire search term appears in total document. \n",
    "df_train['kw3_in_document'] = df_train['keyword_doc3'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
    "# Number of words that appear in search term also appear in total document.\n",
    "df_train['word_in_document'] = df_train['keyword_doc3'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
    "# The ratio of total document word length to search term word length\n",
    "df_train['kw3_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_kw3']\n",
    "# The ratio of total document and search term common word count to search term word count\n",
    "df_train['kw3_ratio_document'] = df_train['word_in_document']/df_train['word_len_of_kw3']\n",
    "\n",
    "\n",
    "# Keyword 4 & Document\n",
    "# Create a new column that combine \"query\" and \"total_document\" \n",
    "df_train['keyword_doc4'] = df_train['keyword_4']+\"\\t\"+df_train['document'] \n",
    "# Number of times the entire search term appears in total document. \n",
    "df_train['kw4_in_document'] = df_train['keyword_doc4'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
    "# Number of words that appear in search term also appear in total document.\n",
    "df_train['word_in_document'] = df_train['keyword_doc4'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
    "# The ratio of total document word length to search term word length\n",
    "df_train['kw4_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_kw4']\n",
    "# The ratio of total document and search term common word count to search term word count\n",
    "df_train['kw4_ratio_document'] = df_train['word_in_document']/df_train['word_len_of_kw4']\n",
    "\n",
    "\n",
    "# Keyword 5 & Document\n",
    "# Create a new column that combine \"query\" and \"total_document\" \n",
    "df_train['keyword_doc5'] = df_train['keyword_5']+\"\\t\"+df_train['document'] \n",
    "# Number of times the entire search term appears in total document. \n",
    "df_train['kw5_in_document'] = df_train['keyword_doc5'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
    "# Number of words that appear in search term also appear in total document.\n",
    "df_train['word_in_document'] = df_train['keyword_doc5'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
    "# The ratio of total document word length to search term word length\n",
    "df_train['kw5_document_len_prop']=df_train['word_len_of_document']/df_train['word_len_of_kw5']\n",
    "# The ratio of total document and search term common word count to search term word count\n",
    "df_train['kw5_ratio_document'] = df_train['word_in_document']/df_train['word_len_of_kw5']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4f8ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>round</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>full_text</th>\n",
       "      <th>document</th>\n",
       "      <th>keyword_1</th>\n",
       "      <th>keyword_2</th>\n",
       "      <th>keyword_3</th>\n",
       "      <th>keyword_4</th>\n",
       "      <th>...</th>\n",
       "      <th>kw3_document_len_prop</th>\n",
       "      <th>kw3_ratio_document</th>\n",
       "      <th>keyword_doc4</th>\n",
       "      <th>kw4_in_document</th>\n",
       "      <th>kw4_document_len_prop</th>\n",
       "      <th>kw4_ratio_document</th>\n",
       "      <th>keyword_doc5</th>\n",
       "      <th>kw5_in_document</th>\n",
       "      <th>kw5_document_len_prop</th>\n",
       "      <th>kw5_ratio_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coronaviru origin</td>\n",
       "      <td>2</td>\n",
       "      <td>6ml0puh8</td>\n",
       "      <td>1</td>\n",
       "      <td>Review\\n\\nMiddle East respiratory syndrome\\nco...</td>\n",
       "      <td>review middl east respiratori syndrom coronavi...</td>\n",
       "      <td>jaffar</td>\n",
       "      <td>phylogenet</td>\n",
       "      <td>evolut</td>\n",
       "      <td>review</td>\n",
       "      <td>...</td>\n",
       "      <td>5298.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>review\\treview middl east respiratori syndrom ...</td>\n",
       "      <td>9</td>\n",
       "      <td>5298.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>coronavirus\\treview middl east respiratori syn...</td>\n",
       "      <td>17</td>\n",
       "      <td>5298.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronaviru origin</td>\n",
       "      <td>2</td>\n",
       "      <td>16rgt4ca</td>\n",
       "      <td>0</td>\n",
       "      <td>Licciardi et al. Pediatric Rheumatology\\n(2020...</td>\n",
       "      <td>licciardi et al. pediatr rheumatolog  2020  18...</td>\n",
       "      <td>licciardi</td>\n",
       "      <td>al</td>\n",
       "      <td>et</td>\n",
       "      <td>000</td>\n",
       "      <td>...</td>\n",
       "      <td>5162.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>000\\tlicciardi et al. pediatr rheumatolog  202...</td>\n",
       "      <td>6</td>\n",
       "      <td>5162.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>oxygen\\tlicciardi et al. pediatr rheumatolog  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5162.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coronaviru origin</td>\n",
       "      <td>2</td>\n",
       "      <td>1bvsn9e8</td>\n",
       "      <td>1</td>\n",
       "      <td>Pathogens and Disease, 77, 2019, ftaa006\\ndoi:...</td>\n",
       "      <td>pathogen and disease  77  2019  ftaa006 doi  1...</td>\n",
       "      <td>ftaa006</td>\n",
       "      <td>77</td>\n",
       "      <td>disease</td>\n",
       "      <td>pathogen</td>\n",
       "      <td>...</td>\n",
       "      <td>3843.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pathogen\\tpathogen and disease  77  2019  ftaa...</td>\n",
       "      <td>10</td>\n",
       "      <td>3843.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>doi\\tpathogen and disease  77  2019  ftaa006 d...</td>\n",
       "      <td>15</td>\n",
       "      <td>3843.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coronaviru respons to weather chang</td>\n",
       "      <td>2</td>\n",
       "      <td>03id5o2g</td>\n",
       "      <td>0</td>\n",
       "      <td>ARTICLE\\nhttps://doi.org/10.1038/s41467-019-13...</td>\n",
       "      <td>articl https //doi.org/10.1038/s41467 019 1365...</td>\n",
       "      <td>articl</td>\n",
       "      <td>https</td>\n",
       "      <td>doi</td>\n",
       "      <td>nucleated</td>\n",
       "      <td>...</td>\n",
       "      <td>5271.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nucleated\\tarticl https //doi.org/10.1038/s414...</td>\n",
       "      <td>1</td>\n",
       "      <td>5271.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>obtain\\tarticl https //doi.org/10.1038/s41467 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5271.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coronaviru respons to weather chang</td>\n",
       "      <td>2</td>\n",
       "      <td>0oqcx0az</td>\n",
       "      <td>1</td>\n",
       "      <td>Science of the Total Environment 701 (2020) 13...</td>\n",
       "      <td>scienc of the total environ 701  2020  134607 ...</td>\n",
       "      <td>scienc</td>\n",
       "      <td>total</td>\n",
       "      <td>environ</td>\n",
       "      <td>content</td>\n",
       "      <td>...</td>\n",
       "      <td>5619.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>content\\tscienc of the total environ 701  2020...</td>\n",
       "      <td>1</td>\n",
       "      <td>5619.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>www\\tscienc of the total environ 701  2020  13...</td>\n",
       "      <td>1</td>\n",
       "      <td>5619.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>coronaviru recoveri</td>\n",
       "      <td>2</td>\n",
       "      <td>bzc7luwj</td>\n",
       "      <td>1</td>\n",
       "      <td>medRxiv preprint doi: https://doi.org/10.1101/...</td>\n",
       "      <td>medrxiv preprint doi  https //doi.org/10.1101/...</td>\n",
       "      <td>doi</td>\n",
       "      <td>https</td>\n",
       "      <td>medrxiv</td>\n",
       "      <td>preprint</td>\n",
       "      <td>...</td>\n",
       "      <td>5432.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>preprint\\tmedrxiv preprint doi  https //doi.or...</td>\n",
       "      <td>31</td>\n",
       "      <td>5432.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ocular\\tmedrxiv preprint doi  https //doi.org/...</td>\n",
       "      <td>3</td>\n",
       "      <td>5432.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>coronaviru recoveri</td>\n",
       "      <td>2</td>\n",
       "      <td>c4bera7b</td>\n",
       "      <td>0</td>\n",
       "      <td>Research in Social and Administrative Pharmacy...</td>\n",
       "      <td>research in social and administr pharmaci 17  ...</td>\n",
       "      <td>administr</td>\n",
       "      <td>research</td>\n",
       "      <td>social</td>\n",
       "      <td>pharmaci</td>\n",
       "      <td>...</td>\n",
       "      <td>2932.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pharmaci\\tresearch in social and administr pha...</td>\n",
       "      <td>61</td>\n",
       "      <td>2932.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>list\\tresearch in social and administr pharmac...</td>\n",
       "      <td>1</td>\n",
       "      <td>2932.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>coronaviru public dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>yq05djrc</td>\n",
       "      <td>1</td>\n",
       "      <td>TRANSLATIONAL KNOWLEDGE MAP OF COVID-19\\nCesar...</td>\n",
       "      <td>translat knowledg map of covid 19 cesar aguado...</td>\n",
       "      <td>victor</td>\n",
       "      <td>aguado</td>\n",
       "      <td>cort</td>\n",
       "      <td>cesar</td>\n",
       "      <td>...</td>\n",
       "      <td>2102.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cesar\\ttranslat knowledg map of covid 19 cesar...</td>\n",
       "      <td>1</td>\n",
       "      <td>2102.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>translat\\ttranslat knowledg map of covid 19 ce...</td>\n",
       "      <td>6</td>\n",
       "      <td>2102.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>coronaviru public dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>z0bkpmpk</td>\n",
       "      <td>1</td>\n",
       "      <td>arXiv:2004.04569v2 [eess.IV] 9 May 2020\\n\\nCOV...</td>\n",
       "      <td>arxiv 2004.04569 volt. 2  eess.iv  9 may 2020 ...</td>\n",
       "      <td>2004</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>pocus</td>\n",
       "      <td>pocovid</td>\n",
       "      <td>...</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pocovid\\tarxiv 2004.04569 volt. 2  eess.iv  9 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pneumonia\\tarxiv 2004.04569 volt. 2  eess.iv  ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>coronaviru public dataset</td>\n",
       "      <td>2</td>\n",
       "      <td>z4q0eyjo</td>\n",
       "      <td>1</td>\n",
       "      <td>medRxiv preprint doi: https://doi.org/10.1101/...</td>\n",
       "      <td>medrxiv preprint doi  https //doi.org/10.1101/...</td>\n",
       "      <td>doi</td>\n",
       "      <td>https</td>\n",
       "      <td>medrxiv</td>\n",
       "      <td>preprint</td>\n",
       "      <td>...</td>\n",
       "      <td>5197.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>preprint\\tmedrxiv preprint doi  https //doi.or...</td>\n",
       "      <td>49</td>\n",
       "      <td>5197.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>02\\tmedrxiv preprint doi  https //doi.org/10.1...</td>\n",
       "      <td>76</td>\n",
       "      <td>5197.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  round    doc_id  relevancy  \\\n",
       "0                      coronaviru origin      2  6ml0puh8          1   \n",
       "2                      coronaviru origin      2  16rgt4ca          0   \n",
       "3                      coronaviru origin      2  1bvsn9e8          1   \n",
       "4    coronaviru respons to weather chang      2  03id5o2g          0   \n",
       "5    coronaviru respons to weather chang      2  0oqcx0az          1   \n",
       "..                                   ...    ...       ...        ...   \n",
       "130                  coronaviru recoveri      2  bzc7luwj          1   \n",
       "131                  coronaviru recoveri      2  c4bera7b          0   \n",
       "132            coronaviru public dataset      2  yq05djrc          1   \n",
       "133            coronaviru public dataset      2  z0bkpmpk          1   \n",
       "134            coronaviru public dataset      2  z4q0eyjo          1   \n",
       "\n",
       "                                             full_text  \\\n",
       "0    Review\\n\\nMiddle East respiratory syndrome\\nco...   \n",
       "2    Licciardi et al. Pediatric Rheumatology\\n(2020...   \n",
       "3    Pathogens and Disease, 77, 2019, ftaa006\\ndoi:...   \n",
       "4    ARTICLE\\nhttps://doi.org/10.1038/s41467-019-13...   \n",
       "5    Science of the Total Environment 701 (2020) 13...   \n",
       "..                                                 ...   \n",
       "130  medRxiv preprint doi: https://doi.org/10.1101/...   \n",
       "131  Research in Social and Administrative Pharmacy...   \n",
       "132  TRANSLATIONAL KNOWLEDGE MAP OF COVID-19\\nCesar...   \n",
       "133  arXiv:2004.04569v2 [eess.IV] 9 May 2020\\n\\nCOV...   \n",
       "134  medRxiv preprint doi: https://doi.org/10.1101/...   \n",
       "\n",
       "                                              document  keyword_1   keyword_2  \\\n",
       "0    review middl east respiratori syndrom coronavi...     jaffar  phylogenet   \n",
       "2    licciardi et al. pediatr rheumatolog  2020  18...  licciardi          al   \n",
       "3    pathogen and disease  77  2019  ftaa006 doi  1...    ftaa006          77   \n",
       "4    articl https //doi.org/10.1038/s41467 019 1365...     articl       https   \n",
       "5    scienc of the total environ 701  2020  134607 ...     scienc       total   \n",
       "..                                                 ...        ...         ...   \n",
       "130  medrxiv preprint doi  https //doi.org/10.1101/...        doi       https   \n",
       "131  research in social and administr pharmaci 17  ...  administr    research   \n",
       "132  translat knowledg map of covid 19 cesar aguado...     victor      aguado   \n",
       "133  arxiv 2004.04569 volt. 2  eess.iv  9 may 2020 ...       2004       arxiv   \n",
       "134  medrxiv preprint doi  https //doi.org/10.1101/...        doi       https   \n",
       "\n",
       "    keyword_3  keyword_4  ... kw3_document_len_prop  kw3_ratio_document  \\\n",
       "0      evolut     review  ...                5298.0                 1.0   \n",
       "2          et        000  ...                5162.0                 1.0   \n",
       "3     disease   pathogen  ...                3843.0                 1.0   \n",
       "4         doi  nucleated  ...                5271.0                 1.0   \n",
       "5     environ    content  ...                5619.0                 1.0   \n",
       "..        ...        ...  ...                   ...                 ...   \n",
       "130   medrxiv   preprint  ...                5432.0                 1.0   \n",
       "131    social   pharmaci  ...                2932.0                 1.0   \n",
       "132      cort      cesar  ...                2102.0                 1.0   \n",
       "133     pocus    pocovid  ...                1311.0                 1.0   \n",
       "134   medrxiv   preprint  ...                5197.0                 1.0   \n",
       "\n",
       "                                          keyword_doc4  kw4_in_document  \\\n",
       "0    review\\treview middl east respiratori syndrom ...                9   \n",
       "2    000\\tlicciardi et al. pediatr rheumatolog  202...                6   \n",
       "3    pathogen\\tpathogen and disease  77  2019  ftaa...               10   \n",
       "4    nucleated\\tarticl https //doi.org/10.1038/s414...                1   \n",
       "5    content\\tscienc of the total environ 701  2020...                1   \n",
       "..                                                 ...              ...   \n",
       "130  preprint\\tmedrxiv preprint doi  https //doi.or...               31   \n",
       "131  pharmaci\\tresearch in social and administr pha...               61   \n",
       "132  cesar\\ttranslat knowledg map of covid 19 cesar...                1   \n",
       "133  pocovid\\tarxiv 2004.04569 volt. 2  eess.iv  9 ...                1   \n",
       "134  preprint\\tmedrxiv preprint doi  https //doi.or...               49   \n",
       "\n",
       "     kw4_document_len_prop  kw4_ratio_document  \\\n",
       "0                   5298.0                 1.0   \n",
       "2                   5162.0                 1.0   \n",
       "3                   3843.0                 1.0   \n",
       "4                   5271.0                 1.0   \n",
       "5                   5619.0                 1.0   \n",
       "..                     ...                 ...   \n",
       "130                 5432.0                 1.0   \n",
       "131                 2932.0                 1.0   \n",
       "132                 2102.0                 1.0   \n",
       "133                 1311.0                 1.0   \n",
       "134                 5197.0                 1.0   \n",
       "\n",
       "                                          keyword_doc5  kw5_in_document  \\\n",
       "0    coronavirus\\treview middl east respiratori syn...               17   \n",
       "2    oxygen\\tlicciardi et al. pediatr rheumatolog  ...                1   \n",
       "3    doi\\tpathogen and disease  77  2019  ftaa006 d...               15   \n",
       "4    obtain\\tarticl https //doi.org/10.1038/s41467 ...                1   \n",
       "5    www\\tscienc of the total environ 701  2020  13...                1   \n",
       "..                                                 ...              ...   \n",
       "130  ocular\\tmedrxiv preprint doi  https //doi.org/...                3   \n",
       "131  list\\tresearch in social and administr pharmac...                1   \n",
       "132  translat\\ttranslat knowledg map of covid 19 ce...                6   \n",
       "133  pneumonia\\tarxiv 2004.04569 volt. 2  eess.iv  ...                3   \n",
       "134  02\\tmedrxiv preprint doi  https //doi.org/10.1...               76   \n",
       "\n",
       "    kw5_document_len_prop  kw5_ratio_document  \n",
       "0                  5298.0                 1.0  \n",
       "2                  5162.0                 1.0  \n",
       "3                  3843.0                 1.0  \n",
       "4                  5271.0                 1.0  \n",
       "5                  5619.0                 1.0  \n",
       "..                    ...                 ...  \n",
       "130                5432.0                 1.0  \n",
       "131                2932.0                 1.0  \n",
       "132                2102.0                 1.0  \n",
       "133                1311.0                 1.0  \n",
       "134                5197.0                 1.0  \n",
       "\n",
       "[134 rows x 43 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a glance on the data set\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a855d2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'round', 'doc_id', 'relevancy', 'full_text', 'document',\n",
       "       'keyword_1', 'keyword_2', 'keyword_3', 'keyword_4', 'keyword_5',\n",
       "       'word_len_of_query', 'word_len_of_document', 'word_len_of_kw1',\n",
       "       'word_len_of_kw2', 'word_len_of_kw3', 'word_len_of_kw4',\n",
       "       'word_len_of_kw5', 'total_info', 'query_in_document',\n",
       "       'word_in_document', 'query_document_len_prop', 'ratio_document',\n",
       "       'keyword_doc1', 'kw1_in_document', 'kw1_document_len_prop',\n",
       "       'kw1_ratio_document', 'keyword_doc2', 'kw2_in_document',\n",
       "       'kw2_document_len_prop', 'kw2_ratio_document', 'keyword_doc3',\n",
       "       'kw3_in_document', 'kw3_document_len_prop', 'kw3_ratio_document',\n",
       "       'keyword_doc4', 'kw4_in_document', 'kw4_document_len_prop',\n",
       "       'kw4_ratio_document', 'keyword_doc5', 'kw5_in_document',\n",
       "       'kw5_document_len_prop', 'kw5_ratio_document'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks all the columns to drop the unnecessary cols for model training\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1dea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yijulee/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4901: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# only keeps the necessary cols for model training\n",
    "df_train.drop(['query','round','doc_id','full_text','document','total_info','keyword_1','keyword_2','keyword_3','keyword_4','keyword_5','query_in_document', 'kw1_ratio_document','kw2_ratio_document','kw3_ratio_document','kw4_ratio_document','kw5_ratio_document','keyword_doc1','keyword_doc2','keyword_doc3','keyword_doc4','keyword_doc5'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84e9034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['relevancy', 'word_len_of_query', 'word_len_of_document',\n",
       "       'word_len_of_kw1', 'word_len_of_kw2', 'word_len_of_kw3',\n",
       "       'word_len_of_kw4', 'word_len_of_kw5', 'word_in_document',\n",
       "       'query_document_len_prop', 'ratio_document', 'kw1_in_document',\n",
       "       'kw1_document_len_prop', 'kw2_in_document', 'kw2_document_len_prop',\n",
       "       'kw3_in_document', 'kw3_document_len_prop', 'kw4_in_document',\n",
       "       'kw4_document_len_prop', 'kw5_in_document', 'kw5_document_len_prop'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a5c90",
   "metadata": {},
   "source": [
    "<h5> 5 - Trains the model and test its accuracy</h5> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "079f4fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest RMSE: 0.4630\n"
     ]
    }
   ],
   "source": [
    "# Training/Prediction\n",
    "# Model 1: Using Random Forest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Separate the relevancy col and the rest of features cols\n",
    "X = df_train.loc[:, df_train.columns != 'relevancy']\n",
    "y = df_train.loc[:, df_train.columns == 'relevancy']\n",
    "\n",
    "# print(X)\n",
    "\n",
    "\n",
    "# Encode the features using one hot encoder since there's strings features\n",
    "# temp = OneHotEncoder().fit_transform(X).toarray()\n",
    "# X = temp\n",
    "\n",
    "# print(X)\n",
    "# print(len(X[0]))\n",
    "\n",
    "# Splits the data into train and test sets with 70% and 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fits the data to a random forest\n",
    "rf = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=0)\n",
    "rf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Makes prediction using random forest\n",
    "y_pred = rf.predict(X_test)\n",
    "# Calculates the root mean squared erros\n",
    "rf_mse = mean_squared_error(y_pred, y_test, squared = False)\n",
    "print('RandomForest RMSE: %.4f' % rf_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c39b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yijulee/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Model 2: Using Support Vector Machine\n",
    "\n",
    "# Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "# Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred2 = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddcda58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6341463414634146\n"
     ]
    }
   ],
   "source": [
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37386c79",
   "metadata": {},
   "source": [
    "<H2> Predicting relevance between input article and query term </H2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faafba2a",
   "metadata": {},
   "source": [
    "<h5> 1 - Converts pdf file (article) to text file to extract the texts from the article</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95071c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading pdf file from the web\n",
    "import requests\n",
    "file_url = \"https://www.medrxiv.org/content/10.1101/2020.04.08.20057661v1.full.pdf\"\n",
    "  \n",
    "r = requests.get(file_url, stream = True)\n",
    "  \n",
    "with open(\"python.pdf\",\"wb\") as pdf:\n",
    "    for chunk in r.iter_content(chunk_size=1024):\n",
    "  \n",
    "         # writing one chunk at a time to pdf file\n",
    "         if chunk:\n",
    "             pdf.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b53453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required modules \n",
    "import PyPDF2 \n",
    "    \n",
    "# creating a pdf file object \n",
    "pdfFileObj = open('python.pdf', 'rb') \n",
    "    \n",
    "# creating a pdf reader object \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "    \n",
    "# printing number of pages in pdf file \n",
    "x = pdfReader.numPages\n",
    "    \n",
    "# creating a page object \n",
    "pageObj = pdfReader.getPage(x-1) \n",
    "    \n",
    "# extracting text from page \n",
    "text = pageObj.extractText()\n",
    "\n",
    "# so we are extracting each page and putting in to the array, and then we are joining them with a space\n",
    "text_doc = []\n",
    "\n",
    "for i in range(0, x):\n",
    "    pageObj = pdfReader.getPage(i)\n",
    "    text = pageObj.extractText()\n",
    "    text_doc.append(text)\n",
    "    \n",
    "complete_text_doc = \" \".join(text_doc)\n",
    "complete_text_doc = complete_text_doc.replace(\"\\n\", \"\")\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1244ed4",
   "metadata": {},
   "source": [
    "<h5> 2 - Inputs processing and prediction <h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee959380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction(query_term, artcle):\n",
    "    data = {'query':[str(query_term)], 'document':[str(article)]}\n",
    "    my_data = pd.DataFrame(data)\n",
    "\n",
    "    # clean the query term and the doc/article\n",
    "    my_data['query'] = my_data['query'].apply(str_stem)\n",
    "    my_data['document'] = my_data['document'].apply(str_stem)\n",
    "\n",
    "    # process all documents in the dataset\n",
    "    corpus = my_data.loc[:,'document']\n",
    "\n",
    "\n",
    "    # Doc must be at least 5 words length\n",
    "    # Creates new cols of keyword 1 to keyword 5 for each row\n",
    "    tf_idf = compute_tf_idf(corpus[0])\n",
    "    features = tf_idf.index[0:5]\n",
    "    my_data.at[0,'keyword_1'] = features[0]\n",
    "    my_data.at[0,'keyword_2'] = features[1]\n",
    "    my_data.at[0,'keyword_3'] = features[2]\n",
    "    my_data.at[0,'keyword_4'] = features[3]\n",
    "    my_data.at[0,'keyword_5'] = features[4]\n",
    "\n",
    "    # Drops the rows with NA values\n",
    "    my_data = my_data.dropna()\n",
    "\n",
    "\n",
    "    # Creates new columns of the features of the query term, doc, and from keyword 1 to keyword 5\n",
    "    my_data['word_len_of_query'] = my_data['query'].apply(lambda x:len(x.split())).astype(np.int64)\n",
    "    my_data['word_len_of_document'] = my_data['document'].apply(lambda x:len(x.split())).astype(np.int64)\n",
    "    my_data['word_len_of_kw1'] = my_data['keyword_1'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
    "    my_data['word_len_of_kw2'] = my_data['keyword_2'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
    "    my_data['word_len_of_kw3'] = my_data['keyword_3'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
    "    my_data['word_len_of_kw4'] = my_data['keyword_4'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
    "    my_data['word_len_of_kw5'] = my_data['keyword_5'].apply(lambda x:len(str(x).split())).astype(np.int64)\n",
    "\n",
    "\n",
    "    # Query & Document\n",
    "    # Create a new column that combine \"query\" and \"total_document\" \n",
    "    my_data['total_info'] = my_data['query']+\"\\t\"+my_data['document'] \n",
    "    # Number of times the entire search term appears in total document. \n",
    "    my_data['query_in_document'] = my_data['total_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\n",
    "    # Number of words that appear in search term also appear in total document.\n",
    "    my_data['word_in_document'] = my_data['total_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "    # The ratio of total document word length to search term word length\n",
    "    my_data['query_document_len_prop']=my_data['word_len_of_document']/my_data['word_len_of_query']\n",
    "    # The ratio of total document and search term common word count to search term word count\n",
    "    my_data['ratio_document'] = my_data['word_in_document']/my_data['word_len_of_query']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Keyword 1 & Document\n",
    "    # Create a new column that combine \"query\" and \"total_document\" \n",
    "    my_data['keyword_doc1'] = my_data['keyword_1']+\"\\t\"+my_data['document'] \n",
    "    # Number of times the entire search term appears in total document. \n",
    "    my_data['kw1_in_document'] = my_data['keyword_doc1'].apply(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
    "    # Number of words that appear in search term also appear in total document.\n",
    "    my_data['word_in_document'] = my_data['keyword_doc1'].apply(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
    "    # The ratio of total document word length to search term word length\n",
    "    my_data['kw1_document_len_prop']=my_data['word_len_of_document']/my_data['word_len_of_kw1']\n",
    "    # The ratio of total document and search term common word count to search term word count\n",
    "    my_data['kw1_ratio_document'] = my_data['word_in_document']/my_data['word_len_of_kw1']\n",
    "\n",
    "\n",
    "    # Keyword 2 & Document\n",
    "    # Create a new column that combine \"query\" and \"total_document\" \n",
    "    my_data['keyword_doc2'] = my_data['keyword_2']+\"\\t\"+my_data['document'] \n",
    "    # Number of times the entire search term appears in total document. \n",
    "    my_data['kw2_in_document'] = my_data['keyword_doc2'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
    "    # Number of words that appear in search term also appear in total document.\n",
    "    my_data['word_in_document'] = my_data['keyword_doc2'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
    "    # The ratio of total document word length to search term word length\n",
    "    my_data['kw2_document_len_prop']=my_data['word_len_of_document']/my_data['word_len_of_kw2']\n",
    "    # The ratio of total document and search term common word count to search term word count\n",
    "    my_data['kw2_ratio_document'] = my_data['word_in_document']/my_data['word_len_of_kw2']\n",
    "\n",
    "\n",
    "    # Keyword 3 & Document\n",
    "    # Create a new column that combine \"query\" and \"total_document\" \n",
    "    my_data['keyword_doc3'] = my_data['keyword_3']+\"\\t\"+my_data['document'] \n",
    "    # Number of times the entire search term appears in total document. \n",
    "    my_data['kw3_in_document'] = my_data['keyword_doc3'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
    "    # Number of words that appear in search term also appear in total document.\n",
    "    my_data['word_in_document'] = my_data['keyword_doc3'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
    "    # The ratio of total document word length to search term word length\n",
    "    my_data['kw3_document_len_prop']=my_data['word_len_of_document']/my_data['word_len_of_kw3']\n",
    "    # The ratio of total document and search term common word count to search term word count\n",
    "    my_data['kw3_ratio_document'] = my_data['word_in_document']/my_data['word_len_of_kw3']\n",
    "\n",
    "\n",
    "    # Keyword 4 & Document\n",
    "    # Create a new column that combine \"query\" and \"total_document\" \n",
    "    my_data['keyword_doc4'] = my_data['keyword_4']+\"\\t\"+my_data['document'] \n",
    "    # Number of times the entire search term appears in total document. \n",
    "    my_data['kw4_in_document'] = my_data['keyword_doc4'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
    "    # Number of words that appear in search term also appear in total document.\n",
    "    my_data['word_in_document'] = my_data['keyword_doc4'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
    "    # The ratio of total document word length to search term word length\n",
    "    my_data['kw4_document_len_prop']=my_data['word_len_of_document']/my_data['word_len_of_kw4']\n",
    "    # The ratio of total document and search term common word count to search term word count\n",
    "    my_data['kw4_ratio_document'] = my_data['word_in_document']/my_data['word_len_of_kw4']\n",
    "\n",
    "\n",
    "    # Keyword 5 & Document\n",
    "    # Create a new column that combine \"query\" and \"total_document\" \n",
    "    my_data['keyword_doc5'] = my_data['keyword_5']+\"\\t\"+my_data['document'] \n",
    "    # Number of times the entire search term appears in total document. \n",
    "    my_data['kw5_in_document'] = my_data['keyword_doc5'].map(lambda x:str_whole_word(str(x).split('\\t')[0],str(x).split('\\t')[1],0))\n",
    "    # Number of words that appear in search term also appear in total document.\n",
    "    my_data['word_in_document'] = my_data['keyword_doc5'].map(lambda x:str_common_word(str(x).split('\\t')[0],str(x).split('\\t')[1]))\n",
    "    # The ratio of total document word length to search term word length\n",
    "    my_data['kw5_document_len_prop']=my_data['word_len_of_document']/my_data['word_len_of_kw5']\n",
    "    # The ratio of total document and search term common word count to search term word count\n",
    "    my_data['kw5_ratio_document'] = my_data['word_in_document']/my_data['word_len_of_kw5']\n",
    "\n",
    "    # Checks all the columns to drop the unnecessary cols for model training\n",
    "    my_data.columns\n",
    "\n",
    "    # only keeps the necessary cols for model training\n",
    "    my_data.drop(['query','document','total_info','keyword_1','keyword_2','keyword_3','keyword_4','keyword_5','query_in_document', 'kw1_ratio_document','kw2_ratio_document','kw3_ratio_document','kw4_ratio_document','kw5_ratio_document','keyword_doc1','keyword_doc2','keyword_doc3','keyword_doc4','keyword_doc5'], axis=1, inplace=True)\n",
    "\n",
    "    x = my_data.loc[:, my_data.columns != 'relevancy']\n",
    "\n",
    "    # Encode the features using one hot encoder since there's strings features\n",
    "    # temp = OneHotEncoder().fit_transform(x).toarray()\n",
    "    # x = temp\n",
    "\n",
    "    # Predict the response for test dataset\n",
    "    rf_y_pred = rf.predict(x)\n",
    "    clf_y_pred = clf.predict(x)\n",
    "\n",
    "    return rf_y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ecb8110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_term = 'test query'\n",
    "article = 'just soMe Sentences in here'\n",
    "compute_prediction(query_term,article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "157208d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from goose3 import Goose\n",
    "# return the text from a url\n",
    "def text_from_url(url):\n",
    "    article = Goose().extract(url=url)\n",
    "    return article.cleaned_text,article.title,article.meta_description\n",
    "    \n",
    "\n",
    "#testing\n",
    "#text_from_url(\"https://www.webmd.com/lung/coronavirus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0623881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def piechart(keywords,weightage):\n",
    "    \n",
    "    if max(weightage) == 0:\n",
    "        keywords[-1] = \"No relevant matching keywords\"\n",
    "        weightage[-1] = 1\n",
    "    print(keywords)\n",
    "    print(weightage)\n",
    "    fig = plt.figure()\n",
    "    x = np.char.array(keywords)\n",
    "    y = np.array(weightage)\n",
    "    colors = ['yellowgreen','red','gold','lightskyblue','pink','lightcoral','blue','white', 'darkgreen','yellow','grey','violet','magenta','cyan']\n",
    "    porcent = 100.*y/y.sum()\n",
    "\n",
    "    patches, texts = plt.pie(y, colors=colors, startangle=90, radius=1.2)\n",
    "    labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(x, porcent)]\n",
    "\n",
    "    sort_legend = True\n",
    "    if sort_legend:\n",
    "        patches, labels, dummy =  zip(*sorted(zip(patches, labels, y),key=lambda x: x[2],reverse=True))\n",
    "\n",
    "    plt.legend(patches, labels, loc='upper left',bbox_to_anchor=(1,1.025),fontsize=8)\n",
    "\n",
    "    res = fig\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0cc81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting date from article\n",
    "#Install dependecies \n",
    "# pip install htmldate \n",
    "# pip install --upgrade htmldate \n",
    "# pip install git+https://github.com/adbar/htmldate.git \n",
    "\n",
    "from htmldate import find_date\n",
    "\n",
    "def return_article_date(url):\n",
    "    return find_date(url)\n",
    "\n",
    "#testing\n",
    "url = 'https://www.webmd.com/lung/coronavirus'\n",
    "#return_article_date(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b45f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def covid_Stats(Country):\n",
    "    page = requests.get(\"https://www.worldometers.info/coronavirus\")\n",
    "    #Instatiating the BeautifulSoup Class\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "    #Where soup is a variable containing the HTML of the webpage\n",
    "    \n",
    "    # Search for the table and extracting it\n",
    "    table = soup.find('table', attrs={'id': 'main_table_countries_today'})\n",
    "    \n",
    "    # extracting all rows\n",
    "    rows = table.find_all(\"tr\", attrs={\"style\": \"\"})\n",
    "    \n",
    "    data = []\n",
    "    for i,item in enumerate(rows):\n",
    "    \n",
    "        if i == 0:\n",
    "        \n",
    "            data.append(item.text.strip().split(\"\\n\")[:13])\n",
    "        \n",
    "        else:\n",
    "            data.append(item.text.strip().split(\"\\n\")[:12])\n",
    "            \n",
    "            \n",
    "    # to suppress future warnings\n",
    "\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "    import pandas as pd\n",
    "    import dask.dataframe as dd\n",
    "\n",
    "    dt = pd.DataFrame(data)\n",
    "    dt = pd.DataFrame(data[1:], columns=data[0][:12]) #Formatting the header\n",
    "    df = dd.from_pandas(dt,npartitions=1)\n",
    "    \n",
    "    \n",
    "    #Extracting the data we need to put on the website\n",
    "    row = df.loc[df['Country,Other'] == Country].head(1)\n",
    "    TotalCases = row['TotalCases'].values[0]\n",
    "    NewCases =  row['NewCases'].values[0]\n",
    "    TotalDeaths = row['TotalDeaths'].values[0]\n",
    "    NewDeaths = row['NewDeaths'].values[0]\n",
    "    TotalRecovered = row['TotalRecovered'].values[0]\n",
    "    \n",
    "    return TotalCases,NewCases,TotalDeaths,NewDeaths,TotalRecovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4c19012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def completeOutput(query_term,url):\n",
    "    \n",
    "    #Assigning query term\n",
    "    query_term = query_term\n",
    "    \n",
    "    article_info = text_from_url(url)\n",
    "    \n",
    "    # extracting text from article\n",
    "    text = article_info[0]\n",
    "    \n",
    "    #extracting article title\n",
    "    title = article_info[1]\n",
    "    \n",
    "    #extracting article abstract\n",
    "    abstract = article_info[2]\n",
    "    \n",
    "    #extracting article date\n",
    "    article_date = return_article_date(article)\n",
    "    \n",
    "    #country you want the stats for\n",
    "    Country = \"Malaysia\"\n",
    "    covidStatus = covid_Stats(Country)\n",
    "    \n",
    "    #extracting covid information for that country \n",
    "    TotalCases = covidStatus[0]\n",
    "    NewCases = covidStatus[1]\n",
    "    TotalDeaths = covidStatus[2]\n",
    "    NewDeaths = covidStatus[3]\n",
    "    TotalRecovered = covidStatus[4]\n",
    "    \n",
    "    \n",
    "    #pie chart \n",
    "    Keywords = ['Covid', 'N95 respirator', 'Antibody', 'Community spread', 'Containment','COVID-19','Ventilator','Cure', 'Hygiene',\n",
    "        'Outbreak', 'Social distancing', 'Quarantine', 'Isolation','Super spreader','Surgical mask','Immunity','SARS-CoV-2''Treatment'\n",
    "        ,'Vaccination','Virus']\n",
    "    Occurences = Counter()\n",
    "    for keyword in Keywords:\n",
    "        occur = text.count(keyword.lower())\n",
    "        Occurences[keyword] = occur\n",
    "    temp = Occurences.most_common(5)\n",
    "    kw_lst = []\n",
    "    occ_lst = []\n",
    "    for kw in temp:\n",
    "        kw_lst.append(kw[0])\n",
    "        occ_lst.append(kw[1])\n",
    "        \n",
    "    pie_chart = piechart(kw_lst,occ_lst)\n",
    "    \n",
    "    #Doing the prediction\n",
    "    prediction = compute_prediction(query_term,text)\n",
    "    \n",
    "    #further evaluate by pre-defined keywords\n",
    "    if sum(occ_lst) < 10 :\n",
    "        prediction_res = 0\n",
    "    else:\n",
    "        if prediction >= 0.9:\n",
    "            prediction_res = 1\n",
    "        else:\n",
    "            prediction_res = 0\n",
    "        \n",
    "    return title,abstract,article_date,TotalCases,NewCases,TotalDeaths,NewDeaths,TotalRecovered,prediction_res,pie_chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2e70eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.settings['recurse'] = True\n",
    "#write it to anywhere in your local machine\n",
    "with open(r\"/Users/yijulee/Document/FIT3164/model/my_model_final\", \"wb\") as dill_file:\n",
    "    dill.dump(completeOutput, dill_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d65a102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Virus', 'Outbreak', 'Covid', 'N95 respirator', 'Antibody']\n",
      "[16, 2, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#testing\n",
    "query_term = 'Coronavirus Origin' # select from the drop down\n",
    "article = 'https://www.webmd.com/lung/coronavirus-history' # get from user\n",
    "output = completeOutput(query_term,article)\n",
    "title = output[0]\n",
    "abstract = output[1]\n",
    "article_date = output[2]\n",
    "TotalCases= output[3]\n",
    "NewCases= output[4]\n",
    "TotalDeaths = output[5]\n",
    "NewDeaths = output[6]\n",
    "TotalRecovered = output[7]\n",
    "prediction = output[8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9a0d1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus History: Origin and Evolution\n",
      "Coronaviruses are a large family of different viruses and have coexisted with humans for a long time. The leap from animals to humans, however, is new. Learn more about coronaviruses here.\n",
      "2021-08-15\n",
      "2,407,382\n",
      "\n",
      "28,138 \n",
      "\n",
      "2,297,289\n",
      "1\n",
      "<bound method Figure.show of <Figure size 432x288 with 1 Axes>>\n"
     ]
    }
   ],
   "source": [
    "print(title)\n",
    "print(abstract)\n",
    "print(article_date)\n",
    "print(TotalCases)\n",
    "print(NewCases)\n",
    "print(TotalDeaths)\n",
    "print(NewDeaths)\n",
    "print(TotalRecovered)\n",
    "print(prediction)\n",
    "print(output[9].show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce2ad612",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[9].savefig('piechart.png',bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
